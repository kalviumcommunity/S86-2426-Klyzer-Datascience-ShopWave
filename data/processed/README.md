# Processed Data

**Purpose:** Cleaned, transformed, and feature-engineered datasets

## Purpose

This folder contains datasets that have been:
- Cleaned (missing values, duplicates, errors)
- Transformed (normalized, encoded, aggregated)
- Feature-engineered (new columns derived from raw data)
- Merged (combined from multiple sources)

## Important Rules

✓ Files here are **generated by scripts**, not manually edited

✓ Can be recreated by running data processing scripts

✓ Document processing steps in notebooks or scripts

## What Goes Here

✅ Store:
- Cleaned datasets ready for analysis
- Feature-engineered datasets
- Merged/joined datasets
- Train/test splits
- Normalized or scaled data

❌ Do NOT store:
- Original raw data (use `data/raw/`)
- Model outputs or predictions (use `outputs/`)
- Visualizations (use `outputs/figures/`)

## Naming Conventions

Use descriptive names that indicate processing:

```
sales_cleaned.csv           # After cleaning
sales_with_features.csv     # After feature engineering
sales_train.csv             # Training set
sales_test.csv              # Test set
customer_sales_merged.csv   # After merging datasets
```

## Example Usage

```python
import pandas as pd

# Load processed data
df = pd.read_csv('../data/processed/sales_cleaned.csv')

# Ready for analysis or modeling
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y)
```

## Reproducibility

To ensure reproducibility:
1. Document processing steps in notebooks
2. Save data processing scripts in `src/`
3. Version control your scripts, not necessarily the data
4. Document data transformations in README or notebooks
