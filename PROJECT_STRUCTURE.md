# Data Science Project Structure

## Overview

This document explains the organization of this Data Science project and provides guidelines for maintaining a clean, scalable folder structure.

---

## Project Structure

```
S86-2426-Klyzer-Datascience-ShopWave/
â”‚
â”œâ”€â”€ data/                   # All datasets
â”‚   â”œâ”€â”€ raw/               # Original, immutable data
â”‚   â””â”€â”€ processed/         # Cleaned, transformed data
â”‚
â”œâ”€â”€ notebooks/             # Jupyter notebooks for analysis
â”‚
â”œâ”€â”€ src/                   # Reusable Python scripts
â”‚
â”œâ”€â”€ outputs/               # Generated results
â”‚   â”œâ”€â”€ figures/          # Plots and visualizations
â”‚   â””â”€â”€ reports/          # Generated reports
â”‚
â”œâ”€â”€ docs/                  # Project documentation
â”‚
â”œâ”€â”€ README.md             # Project overview
â””â”€â”€ requirements.txt      # Python dependencies (if needed)
```

---

## Folder Descriptions

### ğŸ“ `data/`
**Purpose:** Store all project datasets

#### `data/raw/`
- **Original, unmodified source data**
- âš ï¸ **NEVER edit files here**
- Read-only reference data
- Examples: CSVs from downloads, API responses, database exports

#### `data/processed/`
- **Cleaned and transformed datasets**
- Generated by data processing scripts
- Ready for analysis and modeling
- Examples: Cleaned CSVs, feature-engineered data, train/test splits

**Why separate raw and processed?**
- Preserves data integrity
- Enables reproducibility
- Prevents accidental data corruption
- Clear audit trail of transformations

---

### ğŸ““ `notebooks/`
**Purpose:** Jupyter notebooks for exploration and analysis

**What goes here:**
- Exploratory Data Analysis (EDA)
- Data visualization
- Model experimentation
- Analysis documentation
- Milestone demonstrations

**Naming convention:**
- Use descriptive names: `01_data_exploration.ipynb`
- Number notebooks if there's a sequence
- Keep notebooks focused on specific tasks

**Best practices:**
- Restart & Run All before committing
- Clear outputs of sensitive data
- Document reasoning with Markdown
- Keep notebooks concise and focused

---

### ğŸ `src/`
**Purpose:** Reusable Python scripts and modules

**What goes here:**
- Data processing functions
- Feature engineering pipelines
- Model training scripts
- Utility functions
- Custom modules

**Why separate from notebooks?**
- Reusable across multiple notebooks
- Easier to test and debug
- Version control friendly
- Production-ready code

**Example files:**
```
src/
â”œâ”€â”€ data_processing.py      # Data cleaning functions
â”œâ”€â”€ feature_engineering.py  # Feature creation
â”œâ”€â”€ model_training.py       # Model pipelines
â”œâ”€â”€ visualization.py        # Custom plotting functions
â””â”€â”€ utils.py               # General utilities
```

---

### ğŸ“Š `outputs/`
**Purpose:** Store generated results

#### `outputs/figures/`
- All plots and visualizations
- PNG, SVG, PDF formats
- High-resolution images (dpi=300)
- Descriptive filenames

#### `outputs/reports/`
- Generated analysis reports
- Model evaluation summaries
- Exported presentations
- HTML, PDF, Markdown formats

**Why separate outputs?**
- Keep analysis results organized
- Easy to share specific outputs
- Regenerable from source code
- Can be excluded from version control

---

### ğŸ“š `docs/`
**Purpose:** Project documentation

**What goes here:**
- Project overview and goals
- Data dictionaries
- Methodology documentation
- Meeting notes
- Technical specifications
- Reference materials

**Key documents:**
- `data_dictionary.md` - Column descriptions
- `methodology.md` - Analysis approach
- `project_overview.md` - Goals and scope
- `references.md` - Literature and resources

---

## File Organization Best Practices

### âœ… DO:

1. **Use clear, descriptive names**
   ```
   âœ“ sales_analysis_2024.ipynb
   âœ— notebook1.ipynb
   ```

2. **Separate by purpose**
   - Data in `data/`
   - Code in `notebooks/` or `src/`
   - Results in `outputs/`

3. **Keep raw data immutable**
   - Never modify files in `data/raw/`
   - Always create new files in `data/processed/`

4. **Document your structure**
   - Add README files to folders
   - Explain naming conventions
   - Document data sources

5. **Use consistent naming**
   - lowercase with underscores: `my_file.csv`
   - Avoid spaces: `my-file.csv` or `my_file.csv`
   - Be descriptive: `customer_sales_2024.csv`

### âŒ DON'T:

1. **Don't mix file types**
   ```
   âœ— Storing CSVs in notebooks/
   âœ— Storing plots in data/
   âœ— Storing scripts in outputs/
   ```

2. **Don't use generic names**
   ```
   âœ— data.csv
   âœ— output.png
   âœ— test.py
   ```

3. **Don't create deep nesting**
   ```
   âœ— data/raw/2024/january/week1/sales.csv
   âœ“ data/raw/sales_2024_01.csv
   ```

4. **Don't store outputs in data/**
   - Keep sources and results separate

5. **Don't ignore .gitignore**
   - Exclude large data files
   - Exclude generated outputs
   - Exclude sensitive information

---

## Workflow Example

### Typical Data Science Workflow

1. **Data Collection**
   ```
   â†’ Save to data/raw/
   ```

2. **Data Procesing**
   ```python
   # In notebooks/ or src/
   raw_data = pd.read_csv('../data/raw/sales.csv')
   cleaned = clean_data(raw_data)
   cleaned.to_csv('../data/processed/sales_cleaned.csv')
   ```

3. **Analysis**
   ```python
   # In notebooks/
   data = pd.read_csv('../data/processed/sales_cleaned.csv')
   # Perform analysis...
   ```

4. **Generate Outputs**
   ```python
   # Save visualizations
   plt.savefig('../outputs/figures/sales_trends.png')
   
   # Save reports
   results.to_csv('../outputs/reports/analysis_summary.csv')
   ```

5. **Document**
   ```
   â†’ Update docs/ with findings
   â†’ Update README.md
   ```

---

## Path References in Notebooks

When working in `notebooks/`, use relative paths:

```python
# Reading data
df_raw = pd.read_csv('../data/raw/sales.csv')
df_processed = pd.read_csv('../data/processed/sales_cleaned.csv')

# Saving outputs
plt.savefig('../outputs/figures/my_plot.png')

# Importing custom functions
import sys
sys.path.append('../src')
from data_processing import clean_data
```

---

##Git Considerations

### What to Version Control (commit to git):
- âœ… Source code (`src/`, `notebooks/`)
- âœ… Documentation (`docs/`, `README.md`)
- âœ… Configuration files
- âœ… Small sample datasets for testing

### What NOT to Version Control (add to .gitignore):
- âŒ Large data files (`data/`)
- âŒ Generated outputs (`outputs/`)
- âŒ Sensitive information
- âŒ Temporary files (`.ipynb_checkpoints/`)

**Example .gitignore:**
```
# Data files
data/raw/*.csv
data/raw/*.xlsx
data/processed/*.csv

# Outputs
outputs/figures/*.png
outputs/reports/*.pdf

# Jupyter
.ipynb_checkpoints/

# Python
__pycache__/
*.pyc
```

---

## Benefits of Good Structure

### For You:
- âœ… Find files quickly
- âœ… Avoid accidental data corruption
- âœ… Reproduce results easily
- âœ… Debug issues faster
- âœ… Scale projects smoothly

### For Teams:
- âœ… Onboard new members quickly
- âœ… Collaborate without confusion
- âœ… Review work efficiently
- âœ… Share components easily
- âœ… Maintain consistency

### For Projects:
- âœ… Professional appearance
- âœ… Easier maintenance
- âœ… Better reproducibility
- âœ… Clear audit trail
- âœ… Future-proof organization

---

## Extending the Structure

As projects grow, you might add:

```
â”œâ”€â”€ models/          # Saved model files
â”œâ”€â”€ tests/           # Unit tests for src/
â”œâ”€â”€ config/          # Configuration files
â”œâ”€â”€ scripts/         # Command-line scripts
â””â”€â”€ references/      # Literature and papers
```

**Key principle:** Add folders as needed, but keep the core structure simple and consistent.

---

## Summary

**Good project structure is:**
- Simple and intuitive
- Consistent across projects
- Separates concerns clearly
- Scales as projects grow
- Supports collaboration

**Remember:**
- Data in `data/` (raw vs processed)
- Code in `notebooks/` and `src/`
- Results in `outputs/`
- Documentation in `docs/`

---

**This structure is your project's foundation. Maintain it carefully, and it will save you countless hours of confusion and debugging.**
